{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from scipy import misc\n",
    "from time import strftime,strptime\n",
    "import re\n",
    "import math\n",
    "import pymysql\n",
    "import csv\n",
    "import MySQLdb\n",
    "def getimage(url, full_name):\n",
    "\n",
    "    if os.path.isfile(full_name):\n",
    "        return mpimg.imread(full_name)\n",
    "    \n",
    "    try:\n",
    "        f = urllib2.urlopen(url)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    data = f.read()\n",
    "    with open(full_name, \"wb\") as code:\n",
    "        code.write(data)\n",
    "    return mpimg.imread(full_name)\n",
    "\n",
    "\n",
    "def getallimages(panda_name, column_name):\n",
    "    downloaded_images = []\n",
    "    for row in panda_name.loc[panda_name[column_name].notnull(),column_name]:\n",
    "        for image in row:\n",
    "            img = getimage(image['media_url'],image['media_url'].split('/')[-1])\n",
    "            if not(img is None):\n",
    "                downloaded_images.append(img)\n",
    "    return downloaded_images\n",
    "\n",
    "def klout_getId(screenname):\n",
    "    url = 'http://api.klout.com/v2/identity.json/twitter?screenName={0}&key=memp3ncn4qvp6c8guzjcc8dp'.format(screenname)    \n",
    "    try:\n",
    "        return json.load(urllib2.urlopen(url))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def klout_getScore(kloutId):\n",
    "    url = 'http://api.klout.com/v2/user.json/{0}/score?key=memp3ncn4qvp6c8guzjcc8dp'.format(kloutId)\n",
    "    try:\n",
    "        return json.load(urllib2.urlopen(url))\n",
    "    except:\n",
    "        return None\n",
    "def extractImageUrl(cell):\n",
    "    if cell is None:\n",
    "        return None\n",
    "    return cell[0]['media_url']\n",
    "\n",
    "def convertDTToDB(cell):\n",
    "    return strftime('%Y-%m-%d %H:%M:%S', strptime(cell,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "def normalizeTweetText(text):\n",
    "    if text is None or type(text) is float:\n",
    "        return None\n",
    "    return re.sub(r'[^\\w#:@/\\.\\-\\,]', ' ', text)\n",
    "\n",
    "## Note that if we may put NULL for any of the attributes\n",
    "## we should remove the single quotation marks from around them.\n",
    "def ensureDBNull(cell):\n",
    "    try:\n",
    "        return 'NULL' if cell is None else (\"'{0}'\".format(cell))\n",
    "    except:\n",
    "        print(cell)\n",
    "        return 'NULL' if cell is None else (\"'{0}'\".format(cell))\n",
    "\n",
    "def extractHashTags(hashTags):\n",
    "    if hashTags is None or hashTags == []:\n",
    "        return None\n",
    "    mylist = ''\n",
    "    for hashtag in hashTags:\n",
    "        mylist = mylist + ',' + hashtag['text']\n",
    "    return mylist[1:]\n",
    "\n",
    "def getTweetsInsertQuery(tweets):\n",
    "    query = \"\"\n",
    "    for tweet in tweets:\n",
    "        temp = \"REPLACE INTO tweet VALUES('{0}', '{1}', '{2}', '{3}', {4}, \\\n",
    "          '{5}', {6}, {7}, '{8}');\\n\".format(tweet[0],\n",
    "                                     tweet[1],\n",
    "                                     tweet[2],\n",
    "                                     tweet[3],\n",
    "                                     ensureDBNull(tweet[4]),\n",
    "                                     tweet[5],\n",
    "                                     ensureDBNull(tweet[6]),\n",
    "                                     ensureDBNull(tweet[7]),\n",
    "                                     tweet[8])\n",
    "        \n",
    "        query = \"{0}{1}\".format(query, temp)                \n",
    "    return query\n",
    "\n",
    "def getUsersInsertQuery(users):\n",
    "    query = \"\"\n",
    "    for user in users:\n",
    "        temp = \"REPLACE INTO user VALUES('{0}', '{1}', {2}, {3}, \\\n",
    "          '{4}', '{5}', '{6}', {7}, '{8}');\\n\".format(user[0],\n",
    "                                                   user[1],\n",
    "                                                   ensureDBNull(user[2]),\n",
    "                                                   user[3],\n",
    "                                                   user[4],\n",
    "                                                   user[5],\n",
    "                                                   user[6],\n",
    "                                                   ensureDBNull(user[7]),\n",
    "                                                   user[8])\n",
    "\n",
    "        query = \"{0}{1}\".format(query, temp)                \n",
    "    return query\n",
    "\n",
    "def getPlacesInsertQuery(places):\n",
    "    query = \"\"\n",
    "    for place in places:\n",
    "        temp = \"REPLACE INTO place VALUES ('{0}', '{1}', '{2}', '{3}');\\n\".format(place[0],\n",
    "                                                     place[1],\n",
    "                                                     place[2],\n",
    "                                                     place[3])\n",
    "\n",
    "        query = \"{0}{1}\".format(query, temp)\n",
    "    return query\n",
    "\n",
    "def convertNanToNone(text):\n",
    "    try:\n",
    "        if str(text) == 'nan':\n",
    "            return None\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "def getDBInstance():\n",
    "    return None\n",
    "\n",
    "def insertIntoDB(cursor, dataset, table):\n",
    "    if table == \"tweet\":\n",
    "        getInsertionQueries = getTweetsInsertQuery\n",
    "    elif table == \"place\":\n",
    "        getInsertionQueries = getPlacesInsertQuery\n",
    "    elif table == \"user\":\n",
    "        getInsertionQueries = getUsersInsertQuery\n",
    "        \n",
    "    first = 0;\n",
    "    last = 10000;\n",
    "    end = len(dataset.values)\n",
    "    if last > end:\n",
    "        mycursor.execute(getInsertionQueries(dataset.values))\n",
    "    else:\n",
    "        while first < end:\n",
    "            mycursor.execute(getInsertionQueries(dataset.values[first:last]))\n",
    "            first = last;\n",
    "            last += last;\n",
    "            if last > len(dataset.values):\n",
    "                last = end + 1\n",
    "    mycursor.close()\n",
    "    mycursor = mydb.cursor()\n",
    "                \n",
    "def normalizeData(mydata):\n",
    "    ## Choosing the needed columns, removing duplicates,\n",
    "    ## remove all-null rows and renaming the columns\n",
    "    mydata = mydata.drop_duplicates(subset = ['id_str'])\n",
    "    mydata = mydata[['id_str','created_at','text','coordinates.coordinates',\n",
    "                      'entities.media', 'entities.hashtags','lang','user.id_str','user.screen_name',\n",
    "                      'user.location','user.verified','user.followers_count','user.friends_count',\n",
    "                      'user.statuses_count','user.created_at','place.id','place.full_name',\n",
    "                      'place.country_code','place.bounding_box.coordinates']]\n",
    "    mydata = mydata.dropna(axis = 0, how = 'all')\n",
    "    mydata = mydata.loc[mydata['id_str'].notnull()]\n",
    "    mydata = mydata.reset_index(drop = True)\n",
    "    mydata = mydata.rename(index=str, columns={'id_str' : 'tweet.id', 'coordinates.coordinates' : 'tweet.coordinates',\n",
    "                                      'created_at' : 'tweet.created_at', 'text' : 'tweet.text', 'lang' : 'tweet.lang',\n",
    "                                      'entities.media' : 'tweet.media', 'user.id_str' : 'user.id',\n",
    "                                      'entities.hashtags' : 'tweet.hashtags',\n",
    "                                      'place.full_name' : 'place.name', 'place.country_code' : 'place.country',\n",
    "                                      'place.bounding_box.coordinates' : 'place.polygon'})\n",
    "    mydata = mydata.applymap(convertNanToNone)\n",
    "    mydata['tweet.created_at'] = mydata['tweet.created_at'].apply(convertDTToDB)\n",
    "    mydata['user.created_at'] = mydata['user.created_at'].apply(convertDTToDB)\n",
    "    mydata['tweet.coordinates'] = mydata['tweet.coordinates'].apply(convertNanToNone)\n",
    "    mydata['tweet.text'] = mydata['tweet.text'].apply(normalizeTweetText)\n",
    "    mydata['user.location'] = mydata['user.location'].apply(normalizeTweetText)\n",
    "    mydata['tweet.media'] = mydata['tweet.media'].apply(extractImageUrl)\n",
    "    mydata['tweet.hashtags'] = mydata['tweet.hashtags'].apply(extractHashTags)\n",
    "    mydata['tweet.hashtags'] = mydata['tweet.hashtags'].apply(normalizeTweetText)\n",
    "    return mydata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a connection with the DB and make an instance of the cursor\n",
    "mydb = getDBInstance()\n",
    "mydb.autocommit(True)\n",
    "mycursor = mydb.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Upload the data as chunks, apparently the max at once is around 10k insertion for this case.\n",
    "myjson = []\n",
    "myfile = open(\"/media/saed/Data/ubuntu data/data.json\", 'r')\n",
    "a = myfile.readlines()\n",
    "j = 0\n",
    "for i in a:\n",
    "    myjson.append(json.loads(i))\n",
    "    j += 1\n",
    "    if j%10000 == 0 or j == len(a):\n",
    "        \n",
    "        mydata = pd.io.json.json_normalize(myjson)\n",
    "        mydata = normalizeData(mydata)\n",
    "        \n",
    "        ## Tweets\n",
    "        tweets = mydata[['tweet.id','tweet.created_at','tweet.text','user.id','tweet.coordinates','place.id','tweet.media','tweet.hashtags','tweet.lang']]\n",
    "        insertIntoDB(mycursor, tweets, 'tweet')\n",
    "        \n",
    "        ##Users\n",
    "        users = mydata[['user.id','user.screen_name','user.location','user.verified','user.followers_count','user.friends_count','user.statuses_count','user.created_at']]\n",
    "        users = users.drop_duplicates(subset = ['user.id'])\n",
    "        users['klout_score'] = [0.0] * len(users)\n",
    "        insertIntoDB(mycursor, users, 'user')\n",
    "        \n",
    "        ##Places\n",
    "        places = mydata[['place.id','place.name','place.country','place.polygon']]\n",
    "        places = places.drop_duplicates(subset = ['place.id'])\n",
    "        places['place.name'] = places['place.name'].apply(normalizeTweetText)\n",
    "        places = places.dropna(axis = 0, how = 'any')\n",
    "        insertIntoDB(mycursor, places, 'place')\n",
    "        myjson = []\n",
    "        \n",
    "myfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
